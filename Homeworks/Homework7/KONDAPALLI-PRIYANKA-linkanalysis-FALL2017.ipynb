{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1  Create a crawler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../homework7/webcrawling_scraper/webcrawling_scraper/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class WebCrawlingScraperItem(scrapy.Item):\n",
    "    # The source URL\n",
    "    url_from = scrapy.Field()\n",
    "    # The destination URL\n",
    "    url_to = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../homework7/webcrawling_scraper/webcrawling_scraper/spiders/datablogger.py\n",
    "import scrapy\n",
    "from scrapy.linkextractor import LinkExtractor\n",
    "from scrapy.spiders import Rule, CrawlSpider\n",
    "from webcrawling_scraper.items import WebCrawlingScraperItem\n",
    "\n",
    "\n",
    "class DatabloggerSpider(CrawlSpider):\n",
    "    custom_settings = {'DEPTH_LIMIT': 200,}\n",
    "    # The name of the spider\n",
    "    name = \"datablogger\"\n",
    "    # The domains that are allowed (links to other domains are skipped)\n",
    "    allowed_domains = [\"data-blogger.com\"]\n",
    "    # The URLs to start with\n",
    "    start_urls = [\"https://www.data-blogger.com/\"]\n",
    "\n",
    "    # This spider has one rule: extract all (unique and canonicalized) links, follow them and parse them using the parse_items method\n",
    "    rules = [\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                canonicalize=True,\n",
    "                unique=True\n",
    "            ),\n",
    "            follow=True,\n",
    "            callback=\"parse_items\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Method which starts the requests by visiting all URLs specified in start_urls\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url, callback=self.parse, dont_filter=True)\n",
    "\n",
    "    # Method for parsing items\n",
    "    def parse_items(self, response):\n",
    "        # The list of items that are found on the particular page\n",
    "        items = []\n",
    "        # Only extract canonicalized and unique links (with respect to the current page)\n",
    "        links = LinkExtractor(canonicalize=True, unique=True).extract_links(response)\n",
    "        # Now go through all the found links\n",
    "        for link in links:\n",
    "            # Check whether the domain of the URL of the link is allowed; so whether it is in one of the allowed domains\n",
    "            is_allowed = False\n",
    "            for allowed_domain in self.allowed_domains:\n",
    "                if allowed_domain in link.url:\n",
    "                    is_allowed = True\n",
    "            # If it is allowed, create a new item and add it to the list of found items\n",
    "            if is_allowed:\n",
    "                item = WebCrawlingScraperItem()\n",
    "                item['url_from'] = response.url\n",
    "                item['url_to'] = link.url\n",
    "                items.append(item)\n",
    "        # Return all the found items\n",
    "        return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2  Create a Stochastic matrix from its resulting crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_csv(\"webcrawling_scraper/links.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grouping\n",
    "urls_to=data[['url_from','url_to']].groupby(['url_from'],as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_from</th>\n",
       "      <th>url_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.data-blogger.com/</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.data-blogger.com/2016/08/18/scrapi...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.data-blogger.com/2017/02/24/gather...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.data-blogger.com/2017/06/15/fraud-...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.data-blogger.com/author/sarena/</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.data-blogger.com/page/2/</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            url_from  url_to\n",
       "0                      https://www.data-blogger.com/      47\n",
       "1  https://www.data-blogger.com/2016/08/18/scrapi...      32\n",
       "2  https://www.data-blogger.com/2017/02/24/gather...      32\n",
       "3  https://www.data-blogger.com/2017/06/15/fraud-...      16\n",
       "4        https://www.data-blogger.com/author/sarena/      28\n",
       "5               https://www.data-blogger.com/page/2/      45"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would need to create the Stostic matrix to perfrom the analyis\n",
    "<img src=\"https://cs7083.files.wordpress.com/2013/01/b.png\">\n",
    "Now we would need to create the Stostic matrix to perfrom the analyis\n",
    "<img src=\"https://cs7083.files.wordpress.com/2013/01/a.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H=np.zeros((6,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating Stochastic matrix\n",
    "from random import randint\n",
    "for i in range (5):\n",
    "    x=1/float(urls_to.loc[[i]]['url_to'])   \n",
    "    H[i][randint(0, 5)]=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.0212766 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.03125   ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.03125   ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.0625    ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.03571429],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 Pass it through the Page Rank algorithm and provide the list of the top 5 page URLs in your sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    " \n",
    "def pagerank(H):\n",
    "    n= len(H)\n",
    "    w = zeros(n)\n",
    "    rho = 1./n * ones(n);\n",
    "    for i in range(n):\n",
    "      if multiply.reduce(H[i]== zeros(n)):\n",
    "        w[i] = 1\n",
    "    newH = H + outer((1./n * w),ones(n))\n",
    " \n",
    "    theta=0.85\n",
    "    G = (theta * newH) + ((1-theta) * outer(1./n * ones(n), ones(n)))\n",
    "    print (rho)\n",
    "    for j in range(10):\n",
    "        rho = dot(rho,G)\n",
    "        print (rho)\n",
    "        if j==9:\n",
    "            return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16666667  0.16666667  0.16666667  0.16666667  0.16666667  0.16666667]\n",
      "[ 0.05303819  0.05605238  0.05746528  0.04861111  0.04861111  0.05367063]\n",
      "[ 0.01706598  0.01798765  0.01812202  0.01553956  0.01553956  0.01701525]\n",
      "[ 0.00542361  0.00572868  0.00576778  0.00494224  0.00494224  0.00541398]\n",
      "[ 0.00172565  0.0018227   0.001835    0.00157244  0.00157244  0.00172248]\n",
      "[ 0.00054903  0.00057991  0.00058382  0.00050029  0.00050029  0.00054802]\n",
      "[ 0.00017468  0.0001845   0.00018575  0.00015917  0.00015917  0.00017436]\n",
      "[  5.55751637e-05   5.87011694e-05   5.90971467e-05   5.06412422e-05\n",
      "   5.06412422e-05   5.54731877e-05]\n",
      "[  1.76816983e-05   1.86762629e-05   1.88022464e-05   1.61119304e-05\n",
      "   1.61119304e-05   1.76492538e-05]\n",
      "[  5.62557868e-06   5.94200764e-06   5.98209031e-06   5.12614401e-06\n",
      "   5.12614401e-06   5.61525619e-06]\n",
      "[  1.78982442e-06   1.89049891e-06   1.90325155e-06   1.63092515e-06\n",
      "   1.63092515e-06   1.78654023e-06]\n"
     ]
    }
   ],
   "source": [
    "res=pagerank(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    https://www.data-blogger.com/2017/06/15/fraud-...\n",
       "4          https://www.data-blogger.com/author/sarena/\n",
       "5                 https://www.data-blogger.com/page/2/\n",
       "0                        https://www.data-blogger.com/\n",
       "1    https://www.data-blogger.com/2016/08/18/scrapi...\n",
       "2    https://www.data-blogger.com/2017/02/24/gather...\n",
       "Name: url_from, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_to.loc[np.argsort(res)[-10:]]['url_from']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reference:\n",
    "https://www.data-blogger.com/2016/08/18/scraping-a-website-with-python-scrapy/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
